{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dmckRXtuZ3z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip"
      ],
      "metadata": {
        "id": "HkZalRfteA4Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3e0a06a-808d-4a62-c311-83625ebc9294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-10 03:07:08--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 3.5.28.46, 52.217.114.57, 52.217.124.217, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|3.5.28.46|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘annotations_trainval2017.zip.1’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.19M  13.6MB/s    in 19s     \n",
            "\n",
            "2024-12-10 03:07:28 (12.4 MB/s) - ‘annotations_trainval2017.zip.1’ saved [252907541/252907541]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip annotations_trainval2017.zip"
      ],
      "metadata": {
        "id": "nUDR9swyf3uv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6253913-1ce8-4027-80da-bd3a27be600c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  annotations_trainval2017.zip\n",
            "replace annotations/instances_train2017.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace annotations/instances_val2017.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace annotations/captions_train2017.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: m\n",
            "error:  invalid response [m]\n",
            "replace annotations/captions_train2017.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace annotations/captions_val2017.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace annotations/person_keypoints_train2017.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace annotations/person_keypoints_val2017.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://images.cocodataset.org/zips/val2017.zip"
      ],
      "metadata": {
        "id": "VNylviM5QpR1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcdaff9a-031e-4ff2-cab7-ab768fc01e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-10 03:09:28--  http://images.cocodataset.org/zips/val2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 16.182.108.97, 3.5.28.193, 52.217.133.25, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|16.182.108.97|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815585330 (778M) [application/zip]\n",
            "Saving to: ‘val2017.zip.1’\n",
            "\n",
            "val2017.zip.1       100%[===================>] 777.80M  15.7MB/s    in 51s     \n",
            "\n",
            "2024-12-10 03:10:20 (15.1 MB/s) - ‘val2017.zip.1’ saved [815585330/815585330]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip val2017.zip"
      ],
      "metadata": {
        "id": "T4P9OCnyQtt8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57aac56b-fd4f-4463-c955-f5927922793a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  val2017.zip\n",
            "replace val2017/000000212226.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace val2017/000000231527.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace val2017/000000578922.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace val2017/000000062808.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace val2017/000000119038.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace val2017/000000114871.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycocotools ultralytics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_XX2mOmVLPF",
        "outputId": "973aee73-f24f-42b1-fcc3-c02ca25e3c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (2.0.8)\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.48)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools) (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools) (1.26.4)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from pycocotools.coco import COCO\n",
        "from PIL import Image\n",
        "import json\n",
        "import random\n",
        "\n",
        "# Set the device to GPU if available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Define COCO dataset class for custom dataset (detect \"person\" only)\n",
        "class CustomCocoDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, annotation_file, transform=None):\n",
        "        self.root = root\n",
        "        self.coco = COCO(annotation_file)\n",
        "        self.ids = list(self.coco.imgs.keys())\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.ids[idx]\n",
        "        img_info = self.coco.loadImgs(img_id)[0]\n",
        "        path = os.path.join(self.root, img_info['file_name'])\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        # Get annotations for this image\n",
        "        annotations = self.coco.loadAnns(self.coco.getAnnIds(imgIds=img_id))\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in annotations:\n",
        "            # Filter out non-person classes (class ID = 1 for person in COCO)\n",
        "            if ann['category_id'] == 1:  # \"person\" category in COCO\n",
        "                x, y, width, height = ann['bbox']\n",
        "                boxes.append([x, y, x + width, y + height])  # COCO format: [x_min, y_min, x_max, y_max]\n",
        "                labels.append(ann['category_id'])\n",
        "\n",
        "        # If there are no \"person\" annotations, skip this image\n",
        "        if len(boxes) == 0:\n",
        "            return None\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'image_id': torch.tensor([img_id]),\n",
        "            'area': (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]),  # area of bounding boxes\n",
        "            'iscrowd': torch.zeros(len(annotations), dtype=torch.int64)  # assumes no crowd annotations\n",
        "        }\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "# Transformations to apply to the images (e.g., normalization, resizing)\n",
        "transform = T.Compose([\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomRotation(30),\n",
        "    T.ToTensor(),\n",
        "    T.Resize([800, 800]),  # Resize to standard size for Faster R-CNN\n",
        "])\n",
        "\n",
        "# Paths to the dataset and annotations\n",
        "train_images_path = 'val2017'\n",
        "train_annotations_path = 'annotations/instances_val2017.json'\n",
        "\n",
        "# Load the custom dataset\n",
        "train_dataset = CustomCocoDataset(root=train_images_path, annotation_file=train_annotations_path, transform=transform)\n",
        "\n",
        "# Number of images per epoch\n",
        "images_per_epoch = 1000\n",
        "\n",
        "# Create a DataLoader with a batch size of 4 (or adjust to your preference)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# Load a pre-trained Faster R-CNN model\n",
        "model_frcnn = fasterrcnn_resnet50_fpn(pretrained=True).to(device)\n",
        "model_frcnn.eval()  # Set the model to evaluation mode first (to prevent changes to pre-trained weights)\n",
        "\n",
        "# Replace the pre-trained head with a new one for custom dataset (1 class: \"person\")\n",
        "in_features = model_frcnn.roi_heads.box_predictor.cls_score.in_features\n",
        "model_frcnn.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes=2)  # 2 classes: background and person\n",
        "\n",
        "# Move model to GPU\n",
        "model_frcnn.to(device)\n",
        "\n",
        "# Optimizer and learning rate\n",
        "params = [p for p in model_frcnn.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adam(params, lr=1e-4)\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "# Training loop\n",
        "def train_model():\n",
        "    model_frcnn.train()  # Set model to training mode\n",
        "    num_epochs = 10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        random.shuffle(train_dataset.ids)  # Shuffle the dataset\n",
        "\n",
        "        for i in range(0, images_per_epoch, 4):  # Limit to 100 images per epoch, with batch size 4\n",
        "            images, targets = [], []\n",
        "            for j in range(i, min(i + 4, images_per_epoch)):\n",
        "                data = train_dataset[j]\n",
        "                if data:  # Skip images with no \"person\" annotation\n",
        "                    image, target = data\n",
        "                    images.append(image)\n",
        "                    targets.append(target)\n",
        "\n",
        "            images = [image.to(device) for image in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "            if not images or not targets:\n",
        "              continue  # Skip this batch if empty\n",
        "            # Forward pass\n",
        "            loss_dict = model_frcnn(images, targets)\n",
        "\n",
        "            losses = sum(loss for loss in loss_dict.values())  # Total loss\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += losses.item()\n",
        "\n",
        "\n",
        "\n",
        "        # Print training loss after every epoch\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / images_per_epoch}\")\n",
        "\n",
        "        # Update learning rate\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # Save the model after each epoch\n",
        "        torch.save(model_frcnn.state_dict(), f'fasterrcnn_person_epoch_{epoch + 1}.pth')\n",
        "\n",
        "# Run the training loop\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Training Faster R-CNN for 'Person' class only...\")\n",
        "    train_model()"
      ],
      "metadata": {
        "id": "ppQOrCjegFeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffbd4ab0-1ca6-4ef7-a434-2f6549de9fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.45s)\n",
            "creating index...\n",
            "index created!\n",
            "Training Faster R-CNN for 'Person' class only...\n",
            "Epoch 1/10, Loss: 0.14813434050232172\n",
            "Epoch 2/10, Loss: 0.13650826124846935\n",
            "Epoch 3/10, Loss: 0.12810188481211662\n",
            "Epoch 4/10, Loss: 0.12305251525342464\n",
            "Epoch 5/10, Loss: 0.11842656765133143\n",
            "Epoch 6/10, Loss: 0.11965451072901487\n",
            "Epoch 7/10, Loss: 0.12895254723727703\n",
            "Epoch 8/10, Loss: 0.1299205744192004\n",
            "Epoch 9/10, Loss: 0.12156219039857387\n",
            "Epoch 10/10, Loss: 0.11982213146984577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as F\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.ops.boxes import box_iou\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.ops.boxes import box_iou\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_frcnn = fasterrcnn_resnet50_fpn(pretrained=True).to(device)\n",
        "model_frcnn.eval()  # Set the model to evaluation mode first (to prevent changes to pre-trained weights)\n",
        "\n",
        "\n",
        "# Daftar kelas COCO\n",
        "COCO_INSTANCE_CATEGORY_NAMES = [\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
        "    'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant',\n",
        "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
        "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n",
        "    'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
        "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
        "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass',\n",
        "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
        "    'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet',\n",
        "    'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',\n",
        "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]\n",
        "\n",
        "# Fungsi untuk mendeteksi objek hanya \"person\"\n",
        "def detect_person_frcnn(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Preprocessing\n",
        "    image_tensor = F.to_tensor(image).to(device)  # Kirim input ke perangkat yang sesuai\n",
        "\n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        predictions = model_frcnn([image_tensor])\n",
        "\n",
        "    # Filter hasil prediksi hanya untuk label \"person\" (label = 1)\n",
        "    person_predictions = {\n",
        "        \"boxes\": [],\n",
        "        \"scores\": [],\n",
        "        \"labels\": []\n",
        "    }\n",
        "    for box, score, label in zip(predictions[0]['boxes'], predictions[0]['scores'], predictions[0]['labels']):\n",
        "        if label == 1 and score > 0.5:  # Label \"person\" dan skor > 0.5\n",
        "            person_predictions[\"boxes\"].append(box)\n",
        "            person_predictions[\"scores\"].append(score)\n",
        "            person_predictions[\"labels\"].append(label)\n",
        "\n",
        "    # Konversi ke tensor\n",
        "    person_predictions[\"boxes\"] = torch.stack(person_predictions[\"boxes\"]) if person_predictions[\"boxes\"] else torch.tensor([]).to(device)\n",
        "    person_predictions[\"scores\"] = torch.tensor(person_predictions[\"scores\"]).to(device)\n",
        "    person_predictions[\"labels\"] = torch.tensor(person_predictions[\"labels\"]).to(device)\n",
        "\n",
        "    return person_predictions, image  # Mengembalikan prediksi dan gambar asli\n",
        "\n",
        "# Fungsi untuk menampilkan hasil prediksi\n",
        "def plot_predictions(image, predictions):\n",
        "    \"\"\"Visualisasi hasil prediksi.\"\"\"\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(image)\n",
        "\n",
        "    # Jika ada prediksi, gambarkan bounding box\n",
        "    for box, score in zip(predictions[\"boxes\"], predictions[\"scores\"]):\n",
        "        coords = box.cpu().numpy()\n",
        "        x1, y1, x2, y2 = coords\n",
        "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='blue', facecolor='none')\n",
        "        plt.gca().add_patch(rect)\n",
        "        plt.text(x1, y1 - 5, f'Person: {score:.2f}', color='blue', fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Path folder COCO val2017\n",
        "val2017_path = \"val2017\"  # Ubah ke path folder val2017 Anda\n",
        "annotations_path = \"annotations/instances_val2017.json\"  # File JSON ground truth COCO\n",
        "\n",
        "# Load ground truth COCO\n",
        "with open(annotations_path, \"r\") as f:\n",
        "    coco_data = json.load(f)\n",
        "\n",
        "# Fungsi untuk mendapatkan ground truth dari file JSON COCO\n",
        "def get_ground_truth(coco_data, image_id):\n",
        "    annotations = [ann for ann in coco_data[\"annotations\"] if ann[\"image_id\"] == image_id and ann[\"category_id\"] == 1]  # Hanya ambil \"person\"\n",
        "    boxes = []\n",
        "    labels = []\n",
        "    for ann in annotations:\n",
        "        bbox = ann[\"bbox\"]\n",
        "        # COCO format bbox [x, y, width, height] -> [x1, y1, x2, y2]\n",
        "        x1, y1, w, h = bbox\n",
        "        x2 = x1 + w\n",
        "        y2 = y1 + h\n",
        "        boxes.append([x1, y1, x2, y2])\n",
        "        labels.append(ann[\"category_id\"])\n",
        "    return {\"boxes\": torch.tensor(boxes, dtype=torch.float32), \"labels\": torch.tensor(labels)}\n",
        "\n",
        "# Dapatkan daftar file gambar\n",
        "image_files = sorted(os.listdir(val2017_path))[:50]  # Batasi hanya 500 gambar\n",
        "predictions = []\n",
        "ground_truths = []\n",
        "\n",
        "# Prediksi pada 500 gambar\n",
        "for idx, image_file in enumerate(image_files):\n",
        "    image_path = os.path.join(val2017_path, image_file)\n",
        "    image_id = int(image_file.split(\".\")[0])  # Asumsi nama file adalah ID gambar\n",
        "\n",
        "    # Deteksi objek\n",
        "    pred, image = detect_person_frcnn(image_path)\n",
        "    predictions.append(pred)\n",
        "    # Ambil ground truth\n",
        "    gt = get_ground_truth(coco_data, image_id)\n",
        "    ground_truths.append(gt)\n",
        "\n",
        "    # Visualisasi hanya untuk 3 gambar pertama\n",
        "    if idx < 10:\n",
        "        print(f\"Visualizing prediction for {image_file}...\")\n",
        "        plot_predictions(image, pred)\n",
        "\n",
        "    print(f\"Processed {idx + 1}/{len(image_files)}: {image_file}\")\n",
        "\n",
        "def evaluate_person_predictions(predictions, ground_truths, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Menghitung Precision dan Recall untuk label \"person\" (label = 1).\n",
        "    predictions: daftar prediksi (list of dicts dengan 'boxes', 'scores', dan 'labels')\n",
        "    ground_truths: daftar ground truth (list of dicts dengan 'boxes' dan 'labels')\n",
        "    \"\"\"\n",
        "    total_true_positives = 0\n",
        "    total_false_positives = 0\n",
        "    total_ground_truths = 0\n",
        "\n",
        "    for pred, gt in zip(predictions, ground_truths):\n",
        "        # Pastikan ground truth ada di perangkat yang sama dengan prediksi\n",
        "        gt_boxes = gt[\"boxes\"].to(device)  # Pastikan ground truth ada di perangkat yang sama\n",
        "        gt_labels = gt[\"labels\"].to(device)\n",
        "\n",
        "        pred_boxes = pred[\"boxes\"]\n",
        "        pred_scores = pred[\"scores\"]\n",
        "        pred_labels = pred[\"labels\"]\n",
        "\n",
        "        # Hitung IoU antara prediksi dan ground truth\n",
        "        if len(pred_boxes) > 0 and len(gt_boxes) > 0:\n",
        "            iou = box_iou(pred_boxes, gt_boxes)\n",
        "        else:\n",
        "            iou = torch.tensor([]).to(device)\n",
        "\n",
        "        # Match prediksi dan ground truth berdasarkan IoU threshold\n",
        "        matched_gt = set()\n",
        "        true_positives = 0\n",
        "        false_positives = 0\n",
        "\n",
        "        for i, (box, score, label) in enumerate(zip(pred_boxes, pred_scores, pred_labels)):\n",
        "            if score < 0.5:  # Hanya hitung prediksi dengan skor lebih dari 0.5\n",
        "                continue\n",
        "\n",
        "            # Cari ground truth dengan IoU tertinggi untuk prediksi ini\n",
        "            if len(gt_boxes) > 0:\n",
        "                iou_values = iou[i]\n",
        "                max_iou, max_iou_idx = iou_values.max(0)\n",
        "\n",
        "                if max_iou >= iou_threshold and max_iou_idx.item() not in matched_gt and label == 1:\n",
        "                    true_positives += 1\n",
        "                    matched_gt.add(max_iou_idx.item())\n",
        "                else:\n",
        "                    false_positives += 1\n",
        "            else:\n",
        "                false_positives += 1\n",
        "\n",
        "        total_true_positives += true_positives\n",
        "        total_false_positives += false_positives\n",
        "        total_ground_truths += len(gt_boxes)\n",
        "\n",
        "    precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
        "    recall = total_true_positives / total_ground_truths if total_ground_truths > 0 else 0\n",
        "    return precision, recall\n",
        "\n",
        "# Evaluasi hasil\n",
        "# Jika ground truth tersedia, tambahkan evaluasi mAP atau Precision-Recall\n",
        "precision, recall = evaluate_person_predictions(predictions, ground_truths)\n",
        "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
        "\n",
        "# Fungsi untuk menghitung Average Precision (AP) untuk masing-masing gambar\n",
        "def compute_average_precision(predictions, ground_truths, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Menghitung Average Precision (AP) untuk \"person\" (label = 1).\n",
        "    predictions: daftar prediksi (list of dicts dengan 'boxes', 'scores', dan 'labels')\n",
        "    ground_truths: daftar ground truth (list of dicts dengan 'boxes' dan 'labels')\n",
        "    \"\"\"\n",
        "    all_scores = []\n",
        "    all_labels = []\n",
        "    all_detections = []\n",
        "    all_gt_boxes = []\n",
        "\n",
        "    for pred, gt in zip(predictions, ground_truths):\n",
        "        # Ensure ground truth boxes and labels are tensors\n",
        "        gt_boxes = gt[\"boxes\"]\n",
        "        gt_labels = gt[\"labels\"]\n",
        "\n",
        "        # Convert to tensor if they are lists\n",
        "        if isinstance(gt_boxes, list):\n",
        "            gt_boxes = torch.tensor(gt_boxes, dtype=torch.float32)\n",
        "        if isinstance(gt_labels, list):\n",
        "            gt_labels = torch.tensor(gt_labels, dtype=torch.int64)\n",
        "\n",
        "        # Handle empty ground truths\n",
        "        if len(gt_labels) > 0:\n",
        "            person_indices = gt_labels == 1  # Boolean mask for \"person\"\n",
        "            gt_boxes = gt_boxes[person_indices] if len(gt_boxes) > 0 else torch.empty((0, 4))\n",
        "            gt_labels = gt_labels[person_indices]\n",
        "        else:\n",
        "            gt_boxes = torch.empty((0, 4))\n",
        "            gt_labels = torch.empty(0, dtype=torch.int64)\n",
        "\n",
        "        # Add predictions\n",
        "        pred_boxes = pred[\"boxes\"]\n",
        "        pred_scores = pred[\"scores\"]\n",
        "        pred_labels = pred[\"labels\"]\n",
        "\n",
        "        # Store results\n",
        "        all_gt_boxes.append(gt_boxes)\n",
        "        all_detections.append(pred_boxes)\n",
        "        all_scores.append(pred_scores)\n",
        "        all_labels.append(pred_labels)\n",
        "\n",
        "    return all_scores, all_labels, all_detections, all_gt_boxes\n",
        "\n",
        "\n",
        "# Fungsi untuk menghitung mean Average Precision (mAP)\n",
        "def compute_map(all_scores, all_labels, all_detections, all_gt_boxes, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Menghitung mean Average Precision (mAP) pada dataset.\n",
        "    \"\"\"\n",
        "    # Looping melalui setiap gambar untuk menghitung AP\n",
        "    precision_list = []\n",
        "    recall_list = []\n",
        "    for scores, labels, detections, gt_boxes in zip(all_scores, all_labels, all_detections, all_gt_boxes):\n",
        "        # Sorting berdasarkan confidence score\n",
        "        sorted_indices = torch.argsort(scores, descending=True)\n",
        "        sorted_detections = [detections[i] for i in sorted_indices]\n",
        "        sorted_scores = [scores[i] for i in sorted_indices]\n",
        "        sorted_labels = [labels[i] for i in sorted_indices]\n",
        "\n",
        "        tp = np.zeros(len(sorted_detections))\n",
        "        fp = np.zeros(len(sorted_detections))\n",
        "        total_gt = len(gt_boxes)\n",
        "\n",
        "        for idx, det in enumerate(sorted_detections):\n",
        "            # Check if gt_boxes is empty\n",
        "            if gt_boxes.nelement() == 0: # if gt_boxes is empty, skip the comparison\n",
        "                fp[idx] = 1\n",
        "                continue\n",
        "\n",
        "            iou_values = box_iou(det.unsqueeze(0), gt_boxes)\n",
        "            max_iou, max_iou_idx = iou_values.max(0)\n",
        "\n",
        "            # Check if max_iou is a tensor and get its value if so\n",
        "            max_iou_value = max_iou.item() if max_iou.numel() == 1 else max_iou.max().item()\n",
        "\n",
        "            if max_iou_value >= iou_threshold:\n",
        "                tp[idx] = 1\n",
        "            else:\n",
        "                fp[idx] = 1\n",
        "\n",
        "        # Compute precision and recall\n",
        "        tp_cumsum = np.cumsum(tp)\n",
        "        fp_cumsum = np.cumsum(fp)\n",
        "        # Check if tp_cumsum and fp_cumsum are empty before accessing elements\n",
        "        if len(tp_cumsum) > 0 and len(fp_cumsum) > 0:\n",
        "            precision = tp_cumsum / (tp_cumsum + fp_cumsum) if tp_cumsum[-1] + fp_cumsum[-1] > 0 else 0\n",
        "            recall = tp_cumsum / total_gt if total_gt > 0 else 0\n",
        "        else:\n",
        "            precision = 0  # Handle empty cases by setting precision and recall to 0\n",
        "            recall = 0\n",
        "        if not isinstance(recall, (list, np.ndarray)):\n",
        "            recall = [recall]  # Convert single value\n",
        "\n",
        "        precision_list.append(precision)\n",
        "        recall_list.append(recall)\n",
        "\n",
        "    # Average precision at different recall thresholds\n",
        "    average_precision_list = []\n",
        "    for precision, recall in zip(precision_list, recall_list):\n",
        "        # Check if recall has enough elements for interpolation\n",
        "        if len(recall) < 2:  # If recall has fewer than 2 elements, skip interpolation\n",
        "            average_precision = 0  # or assign a default value\n",
        "        else:\n",
        "            # Use interpolation to calculate AP\n",
        "            recall_interp = np.linspace(0, 1, num=11)\n",
        "            precision_interp = np.interp(recall_interp, recall, precision)\n",
        "            average_precision = np.mean(precision_interp)\n",
        "        average_precision_list.append(average_precision)  # Append AP to the list\n",
        "\n",
        "    # Mean Average Precision (mAP)\n",
        "    mAP = np.mean(average_precision_list)\n",
        "    return mAP\n",
        "\n",
        "# Dapatkan skor, label, dan kotak prediksi untuk evaluasi mAP\n",
        "all_scores, all_labels, all_detections, all_gt_boxes = compute_average_precision(predictions, ground_truths)\n",
        "# Tentukan perangkat\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Jika all_scores, all_labels, dll. adalah daftar, konversi setiap elemen menjadi tensor\n",
        "all_scores = [torch.tensor(score).to(device) for score in all_scores]\n",
        "all_labels = [torch.tensor(label).to(device) for label in all_labels]\n",
        "all_detections = [torch.tensor(detection).to(device) for detection in all_detections]\n",
        "all_gt_boxes = [torch.tensor(gt_box).to(device) for gt_box in all_gt_boxes]\n",
        "\n",
        "# Sekarang, Anda dapat melanjutkan dengan perhitungan mAP\n",
        "map_score = compute_map(all_scores, all_labels, all_detections, all_gt_boxes)\n",
        "print(f\"mAP: {map_score:.4f}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R9Q9ntBohmpq",
        "outputId": "f2094bd3-6cb4-4212-8e7b-e26128c7f0ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HkZQ6jUJn1pn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}